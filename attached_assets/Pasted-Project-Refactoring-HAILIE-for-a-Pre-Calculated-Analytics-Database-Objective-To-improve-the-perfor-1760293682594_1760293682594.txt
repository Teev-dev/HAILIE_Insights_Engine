Project: Refactoring HAILIE for a Pre-Calculated Analytics Database
Objective: To improve the performance, scalability, and maintainability of the HAILIE Insights Engine by replacing on-the-fly calculations with a pre-computed analytics database powered by DuckDB.

Phase 1: Setup and Environment
Add New Dependency:

Update your project's requirements.txt file to include the duckdb library.

Ensure scipy is also listed, as it will be used for the correlation calculation.

Update Project Structure:

The generated DuckDB file will be treated as a data asset. Plan for it to be stored in the attached_assets/ directory, alongside the default Excel dataset. Let's name it hailie_analytics.duckdb.

Phase 2: Create the Offline ETL Pipeline
This involves creating a new, standalone Python script. This script will not be part of the Streamlit application itself. It will be run separately whenever the source TSM data is updated.

File: build_analytics_db.py

Goal: Read the raw Excel TSM data, perform all percentile and correlation calculations for the entire dataset, and save the results into a structured DuckDB file.

Steps:

Import Libraries:

Import pandas, duckdb, and scipy.stats.

Import your existing TSMDataProcessor from data_processor.py.

Data Extraction:

Instantiate the TSMDataProcessor.

Use it to load the default 2024 TSM dataset into a clean Pandas DataFrame, just as the current app does.

Data Transformation (Unpivot):

The loaded DataFrame is "wide" (one row per provider, columns TP01-TP12). Convert this into a "long" format using pandas.melt.

The resulting DataFrame (raw_scores_df) should have columns: provider_code, provider_name, year, tp_measure, score.

Calculation 1: Percentiles:

Create an empty list to hold percentile results.

Group the raw_scores_df by tp_measure.

For each measure (e.g., 'TP01', 'TP02'):

Get the series of all scores for that measure.

For each provider's score in that series, calculate its percentile rank using scipy.stats.percentileofscore().

Append the result (provider_code, year, tp_measure, percentile_rank) to your list.

Convert the list of results into a calculated_percentiles_df DataFrame.

Calculation 2: Correlations (Using Spearman's Rank):

Important: This is a statistical method change. The existing app uses Pearson correlation. We will implement the more appropriate Spearman's rank correlation.

Create a "wide" DataFrame of just the scores, indexed by provider_code.

Get the TP01 score series.

Create an empty list to hold correlation results.

Loop through each other measure from TP02 to TP12:

Get the score series for the current measure (e.g., TP02).

Calculate the correlation using scipy.stats.spearmanr(tp01_scores, other_measure_scores). This returns a correlation coefficient and a p-value; you only need the coefficient.

Append the result (year, tp_measure, correlation_with_tp01) to your list.

Convert the list of results into a calculated_correlations_df DataFrame.

Data Loading (into DuckDB):

Establish a connection to the database file: con = duckdb.connect('attached_assets/hailie_analytics.duckdb').

Write each of the three DataFrames to the database, creating or replacing the tables.

Python

con.execute("CREATE OR REPLACE TABLE raw_scores AS SELECT * FROM raw_scores_df")
con.execute("CREATE OR REPLACE TABLE calculated_percentiles AS SELECT * FROM calculated_percentiles_df")
con.execute("CREATE OR REPLACE TABLE calculated_correlations AS SELECT * FROM calculated_correlations_df")
Close the connection: con.close().

Finalize Script:

Wrap the logic in a main function and add a if __name__ == "__main__": block to make it a runnable command-line tool.

Add print statements or logging to show progress (e.g., "Loaded raw data", "Calculating percentiles...", "Writing to DuckDB... Complete.").

Phase 3: Refactor the Streamlit Application
Now, modify the existing application modules to read from the newly created hailie_analytics.duckdb file instead of performing calculations on the fly.

Refactor data_processor.py:

New Role: This module's primary responsibility will shift from processing Excel files to querying the DuckDB database.


Remove: Gut the complex Excel loading, column detection, and data cleaning logic. This logic now lives exclusively in build_analytics_db.py.


Add: Create a function to manage a global, read-only connection to DuckDB.

Add: Create new data-fetching functions that execute simple, fast SQL queries.

get_provider_percentiles(provider_code): SELECT tp_measure, percentile_rank FROM calculated_percentiles WHERE provider_code = ?

get_all_correlations(): SELECT tp_measure, correlation_with_tp01 FROM calculated_correlations

get_provider_exists(provider_code): SELECT 1 FROM raw_scores WHERE provider_code = ? LIMIT 1

get_all_provider_codes(): SELECT DISTINCT provider_code, provider_name FROM raw_scores

Refactor analytics.py:

New Role: This module will now orchestrate calls to the data processor and apply business logic to the pre-calculated data. It will no longer perform heavy statistical calculations.


Remove: Delete the code that calculates percentiles and correlations.



Modify calculate_priority():

Instead of calculating percentiles, it will now call data_processor.get_provider_percentiles(provider_code).

The rest of the logic—calculating "improvement potential" (100 - percentile) and finding the max—will remain the same, but it will operate on the fetched data.


Modify correlation logic:

Instead of calculating correlation, functions will now call data_processor.get_all_correlations() once and use that data.

Refactor app.py:


Modify Core Flow: The main application flow will no longer instantiate TSMAnalytics and TSMDataProcessor in the same way. It will initialize the new data processor (which connects to DuckDB) and then pass that data to the analytics module.


Handle User Uploads: The feature allowing users to upload their own Excel file is now incompatible with the pre-calculated model. You have two options:

Option A (Recommended for MVP): Disable the file uploader. The application will now only work with the default dataset that the analytics cube is built from.

Option B (Complex): To preserve the feature, the app would need to run a temporary, in-memory version of the build_analytics_db.py logic on the uploaded file for that user session. This is significantly more complex and should be treated as a future enhancement.

Review dashboard.py:

This module should require the least amount of change. It receives data structures from analytics.py and renders them. As long as the refactored analytics.py provides data in the same format (e.g., a dictionary with priority info), dashboard.py will work as is. Verify the data contracts between analytics.py and dashboard.py are still met.


Phase 4: New Deployment and Data Update Workflow
Build Step: The build_analytics_db.py script becomes a mandatory build step.

Instructions: Document that whenever a new TSM Excel file is released, an administrator must run python build_analytics_db.py to regenerate the hailie_analytics.duckdb file.

Deployment:

The application is deployed with the hailie_analytics.duckdb file included in its assets.

The Streamlit application itself is now much lighter, as it only does data reads, not heavy processing.

Phase 5: Testing and Validation
Data Validation:

Write a test script to compare the results for 5-10 specific provider codes.

Check that the percentile ranks calculated by the old system match the values stored in the calculated_percentiles table.

Acknowledge and verify that the correlation values will be different (Spearman vs. Pearson). Manually calculate the Spearman correlation for one measure to ensure the ETL script is correct.

Performance Testing:

Measure the application's response time after a provider code is entered in the old system.

Measure the response time in the new system. The improvement should be significant (from seconds to milliseconds). This is your key success metric.

Error Handling:

Test the app by entering an invalid provider code. Ensure the error handling still works correctly.